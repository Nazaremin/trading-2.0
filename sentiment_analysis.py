"""
–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π —Ä—ã–Ω–∫–∞ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π
–í–∫–ª—é—á–∞–µ—Ç:
- –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Twitter/X, Reddit, Telegram
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
- Sentiment scoring –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è
- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ä—ã–Ω–æ—á–Ω—ã–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è–º–∏
"""

import asyncio
import aiohttp
import logging
import re
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import pandas as pd
import numpy as np

# NLP –∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
try:
    import nltk
    from nltk.sentiment import SentimentIntensityAnalyzer
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.stem import WordNetLemmatizer
    import textblob
    from textblob import TextBlob
    NLTK_AVAILABLE = True
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö NLTK
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
    
    try:
        nltk.data.find('vader_lexicon')
    except LookupError:
        nltk.download('vader_lexicon')
        
except ImportError:
    NLTK_AVAILABLE = False
    logging.warning("NLTK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install nltk textblob")

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch")

# API –∫–ª–∏–µ–Ω—Ç—ã
try:
    import tweepy
    TWITTER_AVAILABLE = True
except ImportError:
    TWITTER_AVAILABLE = False
    logging.warning("Tweepy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tweepy")

try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    REDDIT_AVAILABLE = False
    logging.warning("PRAW –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install praw")

try:
    import feedparser
    RSS_AVAILABLE = True
except ImportError:
    RSS_AVAILABLE = False
    logging.warning("Feedparser –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install feedparser")

@dataclass
class SentimentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    sources: List[str] = field(default_factory=lambda: ['news', 'twitter', 'reddit'])
    languages: List[str] = field(default_factory=lambda: ['en', 'ru'])
    update_interval: int = 300  # —Å–µ–∫—É–Ω–¥—ã
    max_posts_per_source: int = 100
    sentiment_threshold: float = 0.1
    relevance_threshold: float = 0.5
    cache_duration: int = 3600  # —Å–µ–∫—É–Ω–¥—ã

@dataclass
class SentimentData:
    """–î–∞–Ω–Ω—ã–µ –æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è—Ö"""
    source: str
    timestamp: datetime
    text: str
    sentiment_score: float  # -1 –¥–æ 1
    confidence: float  # 0 –¥–æ 1
    relevance: float  # 0 –¥–æ 1
    symbols: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AggregatedSentiment:
    """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"""
    symbol: str
    timestamp: datetime
    overall_sentiment: float
    confidence: float
    volume: int  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    sources: Dict[str, float]  # –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    trend: str  # 'bullish', 'bearish', 'neutral'
    strength: str  # 'weak', 'moderate', 'strong'

class SentimentAnalyzer(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    
    def __init__(self, config: SentimentConfig):
        self.config = config
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    @abstractmethod
    async def analyze_text(self, text: str) -> Tuple[float, float]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (sentiment, confidence)"""
        pass
    
    @abstractmethod
    def extract_symbols(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        pass

class VaderSentimentAnalyzer(SentimentAnalyzer):
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ VADER"""
    
    def __init__(self, config: SentimentConfig):
        super().__init__(config)
        if not NLTK_AVAILABLE:
            raise ImportError("NLTK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        self.analyzer = SentimentIntensityAnalyzer()
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
        # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.financial_keywords = {
            'bullish': ['bull', 'bullish', 'buy', 'long', 'pump', 'moon', 'rocket', 'gain', 'profit', 'up', 'rise', 'surge'],
            'bearish': ['bear', 'bearish', 'sell', 'short', 'dump', 'crash', 'loss', 'down', 'fall', 'drop', 'decline'],
            'neutral': ['hold', 'sideways', 'flat', 'stable', 'consolidation']
        }
    
    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–µ–Ω–∏–µ URL
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏ —Ö–µ—à—Ç–µ–≥–æ–≤ (–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤)
        text = re.sub(r'@\w+', '', text)
        text = re.sub(r'#(?![A-Z]{2,5}\b)', '', text)  # –£–¥–∞–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏, –∫—Ä–æ–º–µ —Ç–∏–∫–µ—Ä–æ–≤
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'[^\w\s$#]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text.lower()
    
    async def analyze_text(self, text: str) -> Tuple[float, float]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            processed_text = self.preprocess_text(text)
            
            # VADER –∞–Ω–∞–ª–∏–∑
            scores = self.analyzer.polarity_scores(processed_text)
            
            # –ö–æ–º–ø–∞—É–Ω–¥–Ω—ã–π —Å–∫–æ—Ä –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å
            sentiment = scores['compound']
            
            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            confidence = 1 - scores['neu']
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            financial_boost = self._calculate_financial_sentiment_boost(processed_text)
            sentiment = np.clip(sentiment + financial_boost, -1, 1)
            
            return sentiment, confidence
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π: {e}")
            return 0.0, 0.0
    
    def _calculate_financial_sentiment_boost(self, text: str) -> float:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        words = text.split()
        boost = 0.0
        
        for word in words:
            if word in self.financial_keywords['bullish']:
                boost += 0.1
            elif word in self.financial_keywords['bearish']:
                boost -= 0.1
        
        return np.clip(boost, -0.5, 0.5)
    
    def extract_symbols(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        symbols = []
        
        # –ü–æ–∏—Å–∫ —Ç–∏–∫–µ—Ä–æ–≤ ($SYMBOL –∏–ª–∏ #SYMBOL)
        ticker_pattern = r'[\$#]([A-Z]{2,5})\b'
        matches = re.findall(ticker_pattern, text.upper())
        symbols.extend(matches)
        
        # –ü–æ–∏—Å–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        crypto_pattern = r'\b(BTC|ETH|ADA|DOT|LINK|UNI|DOGE|SHIB|MATIC|SOL|AVAX)\b'
        crypto_matches = re.findall(crypto_pattern, text.upper())
        symbols.extend(crypto_matches)
        
        # –ü–æ–∏—Å–∫ –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä
        forex_pattern = r'\b(EUR|GBP|JPY|CHF|CAD|AUD|NZD)(USD|EUR|GBP|JPY)\b'
        forex_matches = re.findall(forex_pattern, text.upper())
        symbols.extend([''.join(match) for match in forex_matches])
        
        return list(set(symbols))

class TransformerSentimentAnalyzer(SentimentAnalyzer):
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
    
    def __init__(self, config: SentimentConfig):
        super().__init__(config)
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        try:
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="ProsusAI/finbert",
                tokenizer="ProsusAI/finbert"
            )
        except:
            # Fallback –Ω–∞ –æ–±—â—É—é –º–æ–¥–µ–ª—å
            self.sentiment_pipeline = pipeline("sentiment-analysis")
        
        self.vader_analyzer = VaderSentimentAnalyzer(config) if NLTK_AVAILABLE else None
    
    async def analyze_text(self, text: str) -> Tuple[float, float]:
        """–ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
            if len(text) > 512:
                text = text[:512]
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º
            result = self.sentiment_pipeline(text)[0]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Å–∫–æ—Ä
            if result['label'].upper() in ['POSITIVE', 'BULLISH']:
                sentiment = result['score']
            elif result['label'].upper() in ['NEGATIVE', 'BEARISH']:
                sentiment = -result['score']
            else:
                sentiment = 0.0
            
            confidence = result['score']
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å VADER –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if self.vader_analyzer:
                vader_sentiment, vader_confidence = await self.vader_analyzer.analyze_text(text)
                sentiment = (sentiment + vader_sentiment) / 2
                confidence = (confidence + vader_confidence) / 2
            
            return sentiment, confidence
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º: {e}")
            # Fallback –Ω–∞ VADER
            if self.vader_analyzer:
                return await self.vader_analyzer.analyze_text(text)
            return 0.0, 0.0
    
    def extract_symbols(self, text: str) -> List[str]:
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VADER –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤"""
        if self.vader_analyzer:
            return self.vader_analyzer.extract_symbols(text)
        return []

class NewsDataSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: SentimentConfig):
        self.config = config
        self.logger = logging.getLogger("NewsDataSource")
        
        # RSS —Ñ–∏–¥—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        self.rss_feeds = [
            'https://feeds.bloomberg.com/markets/news.rss',
            'https://www.reuters.com/business/finance/rss',
            'https://feeds.finance.yahoo.com/rss/2.0/headline',
            'https://www.marketwatch.com/rss/topstories',
            'https://feeds.cnbc.com/cnbc/ID/100003114/device/rss/rss.html'
        ]
    
    async def fetch_news(self, symbols: List[str] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        if not RSS_AVAILABLE:
            self.logger.warning("RSS –ø–∞—Ä—Å–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return []
        
        news_items = []
        
        for feed_url in self.rss_feeds:
            try:
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries[:self.config.max_posts_per_source // len(self.rss_feeds)]:
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
                    if symbols:
                        text_content = f"{entry.title} {entry.get('summary', '')}"
                        found_symbols = self._extract_symbols_from_text(text_content, symbols)
                        if not found_symbols:
                            continue
                    
                    news_items.append({
                        'source': 'news',
                        'title': entry.title,
                        'content': entry.get('summary', ''),
                        'url': entry.link,
                        'published': entry.get('published_parsed'),
                        'timestamp': datetime.now()
                    })
                    
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {feed_url}: {e}")
        
        return news_items
    
    def _extract_symbols_from_text(self, text: str, target_symbols: List[str]) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        found = []
        text_upper = text.upper()
        
        for symbol in target_symbols:
            if symbol.upper() in text_upper:
                found.append(symbol)
        
        return found

class TwitterDataSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö Twitter/X"""
    
    def __init__(self, config: SentimentConfig, api_credentials: Dict[str, str]):
        self.config = config
        self.logger = logging.getLogger("TwitterDataSource")
        
        if not TWITTER_AVAILABLE:
            raise ImportError("Tweepy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è API
        try:
            auth = tweepy.OAuthHandler(
                api_credentials['consumer_key'],
                api_credentials['consumer_secret']
            )
            auth.set_access_token(
                api_credentials['access_token'],
                api_credentials['access_token_secret']
            )
            
            self.api = tweepy.API(auth, wait_on_rate_limit=True)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Twitter API: {e}")
            self.api = None
    
    async def fetch_tweets(self, symbols: List[str]) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–≤–∏—Ç–æ–≤ –ø–æ —Å–∏–º–≤–æ–ª–∞–º"""
        if not self.api:
            return []
        
        tweets = []
        
        for symbol in symbols:
            try:
                # –ü–æ–∏—Å–∫ —Ç–≤–∏—Ç–æ–≤
                query = f"${symbol} OR #{symbol} -filter:retweets"
                search_results = tweepy.Cursor(
                    self.api.search_tweets,
                    q=query,
                    lang='en',
                    result_type='recent',
                    tweet_mode='extended'
                ).items(self.config.max_posts_per_source // len(symbols))
                
                for tweet in search_results:
                    tweets.append({
                        'source': 'twitter',
                        'id': tweet.id,
                        'text': tweet.full_text,
                        'user': tweet.user.screen_name,
                        'followers': tweet.user.followers_count,
                        'retweets': tweet.retweet_count,
                        'likes': tweet.favorite_count,
                        'timestamp': tweet.created_at,
                        'symbols': [symbol]
                    })
                    
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–≤–∏—Ç–æ–≤ –¥–ª—è {symbol}: {e}")
        
        return tweets

class RedditDataSource:
    """–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö Reddit"""
    
    def __init__(self, config: SentimentConfig, api_credentials: Dict[str, str]):
        self.config = config
        self.logger = logging.getLogger("RedditDataSource")
        
        if not REDDIT_AVAILABLE:
            raise ImportError("PRAW –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Reddit API
        try:
            self.reddit = praw.Reddit(
                client_id=api_credentials['client_id'],
                client_secret=api_credentials['client_secret'],
                user_agent=api_credentials.get('user_agent', 'SentimentBot/1.0')
            )
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Reddit API: {e}")
            self.reddit = None
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–∞–±—Ä–µ–¥–¥–∏—Ç—ã
        self.subreddits = [
            'investing', 'stocks', 'SecurityAnalysis', 'ValueInvesting',
            'cryptocurrency', 'CryptoCurrency', 'Bitcoin', 'ethereum',
            'forex', 'Forex', 'algotrading', 'SecurityAnalysis'
        ]
    
    async def fetch_posts(self, symbols: List[str]) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ Reddit"""
        if not self.reddit:
            return []
        
        posts = []
        
        for subreddit_name in self.subreddits:
            try:
                subreddit = self.reddit.subreddit(subreddit_name)
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –≥–æ—Ä—è—á–∏—Ö –ø–æ—Å—Ç–æ–≤
                for post in subreddit.hot(limit=self.config.max_posts_per_source // len(self.subreddits)):
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                    text_content = f"{post.title} {post.selftext}"
                    found_symbols = self._extract_symbols_from_text(text_content, symbols)
                    
                    if found_symbols or not symbols:
                        posts.append({
                            'source': 'reddit',
                            'id': post.id,
                            'title': post.title,
                            'text': post.selftext,
                            'subreddit': subreddit_name,
                            'score': post.score,
                            'upvote_ratio': post.upvote_ratio,
                            'num_comments': post.num_comments,
                            'timestamp': datetime.fromtimestamp(post.created_utc),
                            'symbols': found_symbols
                        })
                        
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ –∏–∑ r/{subreddit_name}: {e}")
        
        return posts
    
    def _extract_symbols_from_text(self, text: str, target_symbols: List[str]) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        found = []
        text_upper = text.upper()
        
        for symbol in target_symbols:
            if symbol.upper() in text_upper:
                found.append(symbol)
        
        return found

class SentimentAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self, config: SentimentConfig):
        self.config = config
        self.logger = logging.getLogger("SentimentAggregator")
        
        # –í—ã–±–æ—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
        if TRANSFORMERS_AVAILABLE:
            self.sentiment_analyzer = TransformerSentimentAnalyzer(config)
        elif NLTK_AVAILABLE:
            self.sentiment_analyzer = VaderSentimentAnalyzer(config)
        else:
            raise ImportError("–ù–∏ –æ–¥–∏–Ω –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        self.news_source = NewsDataSource(config)
        self.data_sources = {'news': self.news_source}
        
        # –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.sentiment_cache = {}
        self.last_update = {}
    
    def add_twitter_source(self, api_credentials: Dict[str, str]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ Twitter"""
        try:
            self.data_sources['twitter'] = TwitterDataSource(self.config, api_credentials)
            self.logger.info("Twitter –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–±–∞–≤–ª–µ–Ω")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è Twitter –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
    
    def add_reddit_source(self, api_credentials: Dict[str, str]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ Reddit"""
        try:
            self.data_sources['reddit'] = RedditDataSource(self.config, api_credentials)
            self.logger.info("Reddit –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–æ–±–∞–≤–ª–µ–Ω")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è Reddit –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
    
    async def analyze_sentiment_for_symbols(self, symbols: List[str]) -> Dict[str, AggregatedSentiment]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –¥–ª—è —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤"""
        results = {}
        
        for symbol in symbols:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            cache_key = f"{symbol}_{datetime.now().strftime('%Y%m%d%H%M')}"
            if cache_key in self.sentiment_cache:
                results[symbol] = self.sentiment_cache[cache_key]
                continue
            
            # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            all_sentiments = []
            source_sentiments = {}
            
            for source_name, source in self.data_sources.items():
                try:
                    if source_name == 'news':
                        data = await source.fetch_news([symbol])
                    elif source_name == 'twitter':
                        data = await source.fetch_tweets([symbol])
                    elif source_name == 'reddit':
                        data = await source.fetch_posts([symbol])
                    else:
                        continue
                    
                    # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
                    source_sentiment_scores = []
                    
                    for item in data:
                        text = self._extract_text_from_item(item)
                        if text:
                            sentiment, confidence = await self.sentiment_analyzer.analyze_text(text)
                            
                            # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
                            weight = self._calculate_item_weight(item)
                            weighted_sentiment = sentiment * confidence * weight
                            
                            all_sentiments.append(weighted_sentiment)
                            source_sentiment_scores.append(weighted_sentiment)
                    
                    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
                    if source_sentiment_scores:
                        source_sentiments[source_name] = np.mean(source_sentiment_scores)
                    
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {source_name} –¥–ª—è {symbol}: {e}")
            
            # –û–±—â–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è
            if all_sentiments:
                overall_sentiment = np.mean(all_sentiments)
                confidence = min(1.0, len(all_sentiments) / 50)  # –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö = –±–æ–ª—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ –∏ —Å–∏–ª—ã
                trend = self._determine_trend(overall_sentiment)
                strength = self._determine_strength(overall_sentiment, confidence)
                
                aggregated = AggregatedSentiment(
                    symbol=symbol,
                    timestamp=datetime.now(),
                    overall_sentiment=overall_sentiment,
                    confidence=confidence,
                    volume=len(all_sentiments),
                    sources=source_sentiments,
                    trend=trend,
                    strength=strength
                )
                
                # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
                self.sentiment_cache[cache_key] = aggregated
                results[symbol] = aggregated
                
            else:
                # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
                results[symbol] = AggregatedSentiment(
                    symbol=symbol,
                    timestamp=datetime.now(),
                    overall_sentiment=0.0,
                    confidence=0.0,
                    volume=0,
                    sources={},
                    trend='neutral',
                    strength='weak'
                )
        
        return results
    
    def _extract_text_from_item(self, item: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if item['source'] == 'news':
            return f"{item.get('title', '')} {item.get('content', '')}"
        elif item['source'] == 'twitter':
            return item.get('text', '')
        elif item['source'] == 'reddit':
            return f"{item.get('title', '')} {item.get('text', '')}"
        return ""
    
    def _calculate_item_weight(self, item: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–∞ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        weight = 1.0
        
        if item['source'] == 'twitter':
            # –í–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–≤–∏—Ç–∞
            followers = item.get('followers', 0)
            retweets = item.get('retweets', 0)
            likes = item.get('likes', 0)
            
            popularity_score = (followers / 10000) + (retweets / 100) + (likes / 1000)
            weight = min(3.0, 1.0 + popularity_score)
            
        elif item['source'] == 'reddit':
            # –í–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ—Ä–∞ –ø–æ—Å—Ç–∞
            score = item.get('score', 0)
            upvote_ratio = item.get('upvote_ratio', 0.5)
            
            weight = min(2.0, 1.0 + (score / 100) * upvote_ratio)
            
        elif item['source'] == 'news':
            # –ù–æ–≤–æ—Å—Ç–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π –±–∞–∑–æ–≤—ã–π –≤–µ—Å
            weight = 2.0
        
        return weight
    
    def _determine_trend(self, sentiment: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
        if sentiment > self.config.sentiment_threshold:
            return 'bullish'
        elif sentiment < -self.config.sentiment_threshold:
            return 'bearish'
        else:
            return 'neutral'
    
    def _determine_strength(self, sentiment: float, confidence: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞"""
        strength_score = abs(sentiment) * confidence
        
        if strength_score > 0.6:
            return 'strong'
        elif strength_score > 0.3:
            return 'moderate'
        else:
            return 'weak'
    
    def get_sentiment_history(self, symbol: str, hours: int = 24) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫—ç—à–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å–∏–º–≤–æ–ª—É
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        history_data = []
        for cache_key, sentiment_data in self.sentiment_cache.items():
            if (sentiment_data.symbol == symbol and 
                sentiment_data.timestamp >= cutoff_time):
                
                history_data.append({
                    'timestamp': sentiment_data.timestamp,
                    'sentiment': sentiment_data.overall_sentiment,
                    'confidence': sentiment_data.confidence,
                    'volume': sentiment_data.volume,
                    'trend': sentiment_data.trend,
                    'strength': sentiment_data.strength
                })
        
        return pd.DataFrame(history_data).sort_values('timestamp')
    
    def calculate_sentiment_momentum(self, symbol: str, periods: int = 5) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç—É–º–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
        history = self.get_sentiment_history(symbol, hours=periods)
        
        if len(history) < 2:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–æ–π –º–æ–º–µ–Ω—Ç—É–º –∫–∞–∫ —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–Ω–∏–º –∏ –ø–µ—Ä–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        recent_sentiment = history['sentiment'].iloc[-1]
        old_sentiment = history['sentiment'].iloc[0]
        
        return recent_sentiment - old_sentiment

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def demo_sentiment_analysis():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    
    print("üìä –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ê–ù–ê–õ–ò–ó–ê –ù–ê–°–¢–†–û–ï–ù–ò–ô")
    print("=" * 50)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = SentimentConfig(
        sources=['news'],  # –¢–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        max_posts_per_source=20
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞
    aggregator = SentimentAggregator(config)
    
    # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    symbols = ['AAPL', 'TSLA', 'BTC', 'EURUSD']
    
    print("üîç –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤:", symbols)
    
    try:
        results = await aggregator.analyze_sentiment_for_symbols(symbols)
        
        for symbol, sentiment_data in results.items():
            print(f"\nüìà {symbol}:")
            print(f"   –û–±—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment_data.overall_sentiment:.3f}")
            print(f"   –¢—Ä–µ–Ω–¥: {sentiment_data.trend}")
            print(f"   –°–∏–ª–∞: {sentiment_data.strength}")
            print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {sentiment_data.confidence:.3f}")
            print(f"   –û–±—ä–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {sentiment_data.volume}")
            
            if sentiment_data.sources:
                print(f"   –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
                for source, score in sentiment_data.sources.items():
                    print(f"     {source}: {score:.3f}")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        print(f"\nüî¨ –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:")
        sample_text = "Apple stock is showing strong bullish momentum after great earnings. $AAPL to the moon! üöÄ"
        
        if NLTK_AVAILABLE or TRANSFORMERS_AVAILABLE:
            sentiment, confidence = await aggregator.sentiment_analyzer.analyze_text(sample_text)
            symbols_found = aggregator.sentiment_analyzer.extract_symbols(sample_text)
            
            print(f"   –¢–µ–∫—Å—Ç: {sample_text}")
            print(f"   –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {sentiment:.3f}")
            print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
            print(f"   –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {symbols_found}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π: {e}")
    
    print("\n" + "=" * 50)
    print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO)
    
    # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    asyncio.run(demo_sentiment_analysis())

